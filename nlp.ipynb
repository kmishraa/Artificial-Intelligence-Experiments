import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
import string

# Sample text for tokenization
text = "Tokenization is the process of breaking down text into words or sentences. \
        It is an essential step in natural language processing. \
        This code demonstrates how to tokenize text using Python's NLTK library."

# Tokenize the text into sentences
sentences = sent_tokenize(text)

# Tokenize the text into words
words = word_tokenize(text)

# Lowercasing
words_lower = [word.lower() for word in words]

# Remove punctuation
punctuation_table = str.maketrans('', '', string.punctuation)
words_no_punct = [word.translate(punctuation_table) for word in words_lower]

# Remove stopwords
stop_words = set(stopwords.words('english'))
words_filtered = [word for word in words_no_punct if word not in stop_words]

# Print tokenized sentences
print("Tokenized sentences:")
for sentence in sentences:
    print(sentence)

# Print tokenized words
print("\nTokenized words:")
print(words_filtered)
